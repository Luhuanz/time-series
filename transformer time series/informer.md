# informer

**经常用的两种经典传统算法** ：Prophet 和 Arima。这两种算法不合适做长序列和多标签输出任务。

**论文三大核心：**attention 需要计算的更快。decoder 部分需要一次性输出所有预测。堆叠 encoder 也要更快。

经实验表明：在长序列中，不是每一个位置的 attention 都是重要的。对于每一个 Q，只有一小部分的 K 是与其有较强的关系，大部分的点之间是没有关系的，内积值为 0。下图大部分都是黑色区域

经实验表明：有用的、能干活的 Q 分布较有特点，不起作用的 Q 类似于均匀分布

![img](https://img-blog.csdnimg.cn/edd47d449c994861b1c2ccc786c22360.png)

 如何筛选出这些不起作用的 Q 呢，让每个 Q 计算与均匀分布之间的差异（为什么计算与均匀分布之间的差异，因为不起作用的 Q 的分布就近似于均匀分布），差异越大的表示这些 Q 越积极，越有用。

**Probattention 计算方法（具体怎么筛选出这些有用的 Q）**：

从上面图中可以看出，纵坐标的一个 Q 需要与所有的 K 做计算，但其实是不需要的，这样做增大了计算量，只需要从所有的 K 中随机选出一部分做计算即可，就能知道右侧曲线图的大概，就能知道这个 Q 是干活的还是不干活的。

假设输入序列长度为 96，$x_1...x_{96}$，首先在$ K$ 中做采样，随机选 25 个 K，现在目的是要选出一些重要的 $Q$，本来一个 $Q$ 是要与所有的 96 个 K 做计算，但现在只需要跟随机选出来的部分 $K$ 做计算即可，源码中输出结果 32，8，96，25，表示 batch 为 32，8 头，96 个 Q，25 个 K，即 96 个 Q 分别与 25 个 K 做内积计算。计算之后，结果是 $Q_1K_1,Q_1K_2,Q_1K_3...Q_1K_{25}$， $Q_2K_1,Q_2K_2,Q_2K_3,...Q_2K_{25}$ 等，接下来计算与均匀分布之间的差异，**每一个 Q 需要从 25 个结果中选出最大值与均匀分布做差，****看差值大小，插值越大越有用**。计算完差异之后，进行排序，从大到小，只要前 25 名的 Q，把这 25 个有价值的 Q 选出来之后，之后就正常计算了，上面所有步骤的目的就是选出这有价值的 25 个 Q。最后得到的 QK 内积为 32，8，25，96，batch 为 32，8 头，25 个 Q，每个 Q 与所有的 96 个 K 做内积计算，之后做特征重构，没被选中的 Q 不再计算 attention 值，将其值直接令为均值 1/96V1+1/96V2+...，剩下的没用的没被选中的 Q 算不算都是跟均匀分布差不多，所以就直接将其值设为均值。选中的 25 个 Q 会更新，其余为平均向量
$$
M\left(\mathbf{q}_i, \mathbf{K}\right)=\ln \sum_{j=1}^{L_K} e^{\frac{\mathbf{q}_i \mathbf{k}_j^{\top}}{\sqrt{d}}}-\frac{1}{L_K} \sum_{j=1}^{L_K} \frac{\mathbf{q}_i \mathbf{k}_j^{\top}}{\sqrt{d}}
$$
其中第一项是$q_i$对于所有的 key 的 Log-Sum-Exp (LSE)，第二项是它们的算数平均值。

基于上面的评价方式，就可以得到 ProbSparse self-attetion 的公式，即：


$$
\mathcal{A}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{Softmax}\left(\frac{\overline{\mathbf{Q}} \mathbf{K}^{\top}}{\sqrt{d}}\right) \mathbf{V}
$$


![image-20230413090732500](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230413090732500.png)

所以，最终筛选出有用的 Q 是用的什么方法？首先随机选部分 K 与所有的 Q 计算注意力，再从每一个 Q 中的所有 QK 中选出最大值与均匀分布做差，之后排序，选出前 25 个，这 25 个 Q 就是有用的，这 25 个正常跟 96 个 K 计算注意力，剩下的没被选中的，无用处的 Q 直接设置为均值即可，这样就可以减少计算量，只计算实际有用的。